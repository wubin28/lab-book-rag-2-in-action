## Me:

```
((.venv) ) ➜  ch01 (main) ✗)python conversational_rag_with_memory_local.py
```

## Cursor with claude sonnet 4.5:

```
Please enter your DeepSeek API key:

Loading local embedding model (first time may take a moment)...
/Users/binwu/OOR-local/katas/lab-book-rag-2-in-action/ch01/conversational_rag_with_memory_local.py:21: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.
  embeddings = HuggingFaceEmbeddings(
modules.json: 100%|███| 349/349 [00:00<00:00, 693kB/s]
config_sentence_transformers.json: 100%|█| 116/116 [00
README.md: 10.5kB [00:00, 16.3MB/s]
sentence_bert_config.json: 100%|█| 53.0/53.0 [00:00<00
config.json: 100%|███| 612/612 [00:00<00:00, 3.76MB/s]
model.safetensors: 100%|█| 90.9M/90.9M [00:55<00:00, 1
tokenizer_config.json: 100%|█| 350/350 [00:00<00:00, 6
vocab.txt: 232kB [00:00, 2.57MB/s]
tokenizer.json: 466kB [00:00, 8.71MB/s]
special_tokens_map.json: 100%|█| 112/112 [00:00<00:00,
config.json: 100%|████| 190/190 [00:00<00:00, 592kB/s]
Building vector store...
Vector store ready!
/Users/binwu/OOR-local/katas/lab-book-rag-2-in-action/ch01/conversational_rag_with_memory_local.py:29: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/
  memory = ConversationBufferMemory(

Querying the conversational RAG system...

Query 1: What was RAG 1.0 designed to do?
/Users/binwu/OOR-local/katas/lab-book-rag-2-in-action/ch01/conversational_rag_with_memory_local.py:49: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain-classic 0.1.0 and will be removed in 1.0. Use `invoke` instead.
  response1 = qa_chain.run("What was RAG 1.0 designed to do?")
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Response 1: Based on the provided context, RAG 1.0 was designed to use **static retrieval for each query**. This means it would fetch information from a knowledge source in a fixed, one-time manner for every new question, without the ability to learn from previous interactions or refine its search dynamically.

Query 2: And how does RAG 2.0 improve that?
Response 2: Based on the provided context, RAG 2.0 improves upon RAG 1.0's static retrieval approach in two key ways:

1.  **It enables feedback loops.** This allows the system to learn from its interactions and refine its retrieval process over time, moving beyond a single, fixed retrieval step for each query.
2.  **It integrates memory.** This means the system can retain and utilize information from previous interactions to provide more contextually relevant and consistent responses.

In essence, RAG 2.0 shifts from a one-time, static lookup of information to a more dynamic and adaptive process, which, as the context mentions with LangChain, leads to better accuracy.
```