## Me (on WSL2 Ubuntu):

```
(.venv) ➜  ch01 git:(main) ✗ python conversational_rag_with_memory_local.py
```

## Output:

```
Please enter your DeepSeek API key:

Loading local embedding model (first time may take a moment)...
/home/ben/OOR/katas/lab-book-rag-2-in-action/ch01/conversational_rag_with_memory_local.py:21: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.
  embeddings = HuggingFaceEmbeddings(
Building vector store...
Vector store ready!
/home/ben/OOR/katas/lab-book-rag-2-in-action/ch01/conversational_rag_with_memory_local.py:29: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/
  memory = ConversationBufferMemory(

Querying the conversational RAG system...

Query 1: What was RAG 1.0 designed to do?
/home/ben/OOR/katas/lab-book-rag-2-in-action/ch01/conversational_rag_with_memory_local.py:49: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain-classic 0.1.0 and will be removed in 1.0. Use `invoke` instead.
  response1 = qa_chain.run("What was RAG 1.0 designed to do?")
Response 1: Based on the context provided, RAG 1.0 was designed to use static retrieval for each query.

Query 2: And how does RAG 2.0 improve that?
Response 2: Based on the context provided, RAG 2.0 improves upon the static retrieval approach of RAG 1.0 in two key ways:

1.  **It enables feedback loops.** Unlike RAG 1.0, which performed a single, fixed retrieval for each query, RAG 2.0 can use the results or output of one step to refine and improve the next. This creates a more dynamic and adaptive retrieval process.
2.  **It integrates memory.** This allows the system to learn from past interactions, making its responses more contextually relevant and accurate over time.

This shift from a static to a dynamic process, as also supported by LangChain's capabilities for dynamic context refinement, allows RAG 2.0 to provide more accurate and nuanced answers.
```
